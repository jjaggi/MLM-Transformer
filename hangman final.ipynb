{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# For AMD GPU support via DirectML:\n",
    "import torch_directml\n",
    "device = torch_directml.device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Global Constants & Vocabulary Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Define our “top‐11” letters (chosen because they capture ~70% of letter frequency)\n",
    "TOP_11_LETTERS = [\"E\", \"I\", \"A\", \"R\", \"N\", \"O\", \"S\", \"T\", \"L\", \"C\", \"U\"]\n",
    "ALL_LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Special tokens: PAD and MASK (we do not use UNK in this challenge)\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "\n",
    "# Create token-to-index mapping (we use only the provided vocabulary)\n",
    "token_to_idx = {PAD_TOKEN: 0, MASK_TOKEN: 1}\n",
    "start_idx = 2\n",
    "for ch in ALL_LETTERS:\n",
    "    token_to_idx[ch] = start_idx\n",
    "    start_idx += 1\n",
    "vocab_size = len(token_to_idx)  # e.g. 28 tokens (2 specials + 26 letters)\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "PAD_IDX = token_to_idx[PAD_TOKEN]\n",
    "MAX_LENGTH = 32  # fixed input length for our MLM\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Transformer Model Definition (MLM with Negative Constraints)\n",
    "# =============================================================================\n",
    "\n",
    "class MiniTransformerMLMTop11(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, max_len=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Embedding for tokens and positions.\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        # Linear projection to embed the 11-d negative vector.\n",
    "        self.neg_embed = nn.Linear(11, d_model)\n",
    "        \n",
    "        # Transformer encoder layers (batch_first=True)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=256, \n",
    "            dropout=dropout, \n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final classification layer to predict logits over the vocabulary.\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, inp, neg_info):\n",
    "        \"\"\"\n",
    "        inp: Tensor of shape (batch, seq_len) with token IDs.\n",
    "        neg_info: Tensor of shape (batch, 11) with the binary negative vector.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inp.shape\n",
    "        \n",
    "        # Get token embeddings and add positional embeddings.\n",
    "        tok_emb = self.token_embed(inp)               # (batch, seq_len, d_model)\n",
    "        positions = torch.arange(seq_len, device=inp.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embed(positions)             # (1, seq_len, d_model)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Project the 11-d negative vector and add uniformly to each token position.\n",
    "        neg_vec = self.neg_embed(neg_info)              # (batch, d_model)\n",
    "        neg_vec = neg_vec.unsqueeze(1).expand(-1, seq_len, -1)  # (batch, seq_len, d_model)\n",
    "        x = x + neg_vec\n",
    "        \n",
    "        # Pass through transformer encoder.\n",
    "        x = self.transformer(x)                         # (batch, seq_len, d_model)\n",
    "        logits = self.fc_out(x)                         # (batch, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Heuristic Solver (Using Only the Provided Training File)\n",
    "# =============================================================================\n",
    "\n",
    "class ConfidenceBasedHangmanSolver:\n",
    "    def __init__(self, training_file):\n",
    "        # Load training words ONLY from the provided file.\n",
    "        with open(training_file, 'r', encoding='utf-8') as f:\n",
    "            words = [line.strip().lower() for line in f if line.strip() and line.strip().isalpha()]\n",
    "        self.training_words = set(words)  # Used to avoid final guesses that come from training\n",
    "        self.guessed_letters = set()\n",
    "        self.incorrect = set()  # Letters guessed that are not in the target word\n",
    "        self.current_pattern = None  # Current masked pattern (e.g., \"a__le\")\n",
    "        self.guess_count = 0\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.guessed_letters = set()\n",
    "        self.incorrect = set()\n",
    "        self.current_pattern = None\n",
    "        self.guess_count = 0\n",
    "\n",
    "    def get_length_based_guess(self):\n",
    "        \"\"\"\n",
    "        Basic heuristic: for example, try letters from the prioritized top 11 in order.\n",
    "        (You could enhance this by using precomputed frequency per word-length.)\n",
    "        \"\"\"\n",
    "        for c in \"eaiornstlcu\":\n",
    "            if c not in self.guessed_letters:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def get_global_fallback(self):\n",
    "        # Fallback: simply choose the first unguessed letter from the alphabet.\n",
    "        for c in string.ascii_lowercase:\n",
    "            if c not in self.guessed_letters:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def get_negative_vector(self):\n",
    "        \"\"\"\n",
    "        Build the 11-d binary vector:\n",
    "          For each letter in TOP_11_LETTERS, 1 if it has been guessed (incorrectly) and is not in the target.\n",
    "        \"\"\"\n",
    "        return [1 if letter.lower() in self.incorrect else 0 for letter in TOP_11_LETTERS]\n",
    "\n",
    "    def update_state(self, pattern, target_word):\n",
    "        \"\"\"\n",
    "        Update solver state after each guess.\n",
    "          pattern: the current masked word (e.g., \"a__le\")\n",
    "          target_word: the true word (if available during simulation/training)\n",
    "        \"\"\"\n",
    "        self.current_pattern = pattern.lower().replace(\" \", \"\")\n",
    "        self.guess_count += 1\n",
    "        # Update incorrect letters (if a letter was guessed but not in target_word)\n",
    "        for letter in self.guessed_letters:\n",
    "            if letter not in target_word:\n",
    "                self.incorrect.add(letter)\n",
    "\n",
    "    def hybrid_guess(self, pattern, target_word=None, transformer_model=None):\n",
    "        \"\"\"\n",
    "        Return the next guessed letter using a hybrid strategy:\n",
    "          - If the clean pattern (without spaces) is longer than MAX_LENGTH,\n",
    "            use heuristic only.\n",
    "          - For the first two guesses, use heuristic.\n",
    "          - Otherwise, if a transformer_model is provided, use its prediction.\n",
    "          - In all cases, update self.guessed_letters accordingly.\n",
    "        \"\"\"\n",
    "        clean_pattern = pattern.lower().replace(\" \", \"\")\n",
    "        \n",
    "        # If word length > MAX_LENGTH, MLM cannot process it so use heuristic.\n",
    "        if len(clean_pattern) > MAX_LENGTH:\n",
    "            guess = self.get_length_based_guess() or self.get_global_fallback()\n",
    "            self.guessed_letters.add(guess)\n",
    "            return guess\n",
    "\n",
    "        # For the first two guesses, use the heuristic.\n",
    "        if self.guess_count < 2:\n",
    "            guess = self.get_length_based_guess() or self.get_global_fallback()\n",
    "            self.guessed_letters.add(guess)\n",
    "            return guess\n",
    "\n",
    "        # Otherwise, use the MLM-based approach if the transformer model is available.\n",
    "        if transformer_model is not None:\n",
    "            neg_vector = self.get_negative_vector()  # 11-d list (binary)\n",
    "            # Convert pattern into token IDs\n",
    "            token_ids = []\n",
    "            for ch in clean_pattern:\n",
    "                if ch == '_':\n",
    "                    token_ids.append(token_to_idx[MASK_TOKEN])\n",
    "                else:\n",
    "                    token_ids.append(token_to_idx[ch.upper()])\n",
    "            # Pad/truncate to MAX_LENGTH\n",
    "            if len(token_ids) < MAX_LENGTH:\n",
    "                token_ids += [token_to_idx[PAD_TOKEN]] * (MAX_LENGTH - len(token_ids))\n",
    "            else:\n",
    "                token_ids = token_ids[:MAX_LENGTH]\n",
    "            \n",
    "            input_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "            neg_tensor = torch.tensor([neg_vector], dtype=torch.float).to(device)\n",
    "            transformer_model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = transformer_model(input_tensor, neg_tensor)  # (1, seq_len, vocab_size)\n",
    "            \n",
    "            # Find masked positions in the input\n",
    "            mask_positions = [i for i, tid in enumerate(token_ids) if tid == token_to_idx[MASK_TOKEN]]\n",
    "            if not mask_positions:\n",
    "                # No masked positions—fallback to global.\n",
    "                guess = self.get_global_fallback()\n",
    "                self.guessed_letters.add(guess)\n",
    "                return guess\n",
    "            \n",
    "            # Sum probabilities over all masked positions.\n",
    "            probs_total = collections.Counter()\n",
    "            for pos in mask_positions:\n",
    "                prob = torch.softmax(logits[0, pos], dim=-1)\n",
    "                for i in range(2, vocab_size):  # Ignore PAD and MASK indices 0 and 1.\n",
    "                    letter = idx_to_token[i]\n",
    "                    probs_total[letter.lower()] += prob[i].item()\n",
    "            # Choose the letter with the highest summed probability that has not been guessed.\n",
    "            for letter, _ in probs_total.most_common():\n",
    "                if letter not in self.guessed_letters:\n",
    "                    self.guessed_letters.add(letter)\n",
    "                    return letter\n",
    "        \n",
    "        # If no transformer model is provided or MLM branch fails, fallback.\n",
    "        guess = self.get_global_fallback()\n",
    "        self.guessed_letters.add(guess)\n",
    "        return guess\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Hybrid Hangman API (Using our Heuristic-MLM Solver)\n",
    "# =============================================================================\n",
    "\n",
    "class HangmanAPIHybrid:\n",
    "    def __init__(self, access_token, training_file, transformer_model, timeout=2000):\n",
    "        self.hangman_url = \"https://trexsim.com/trexsim/hangman\"\n",
    "        self.access_token = access_token\n",
    "        self.session = requests.Session()\n",
    "        self.timeout = timeout\n",
    "        # hybrid solver\n",
    "        self.solver = ConfidenceBasedHangmanSolver(training_file)\n",
    "        self.transformer_model = transformer_model\n",
    "\n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        self.solver.reset_game()  # Reset solver state at game start.\n",
    "        response = self.request(\"/new_game\", {\"practice\": practice})\n",
    "        if response.get(\"status\") == \"approved\":\n",
    "            game_id = response.get(\"game_id\")\n",
    "            word = response.get(\"word\")\n",
    "            tries_remains = response.get(\"tries_remains\")\n",
    "            if verbose:\n",
    "                print(f\"Game Started: ID={game_id}, Tries Remaining={tries_remains}, Word='{word}'\")\n",
    "            return game_id, word\n",
    "        else:\n",
    "            print(\"Failed to start a new game\")\n",
    "            return None, None\n",
    "\n",
    "    def guess(self, word):\n",
    "        \"\"\"\n",
    "        Given the current masked word (e.g., \"_ A _ E\"), use the hybrid solver to choose a guess.\n",
    "        \"\"\"\n",
    "        # Here, we do not update the solver with the true word since during play we don't have it.\n",
    "        return self.solver.hybrid_guess(word, transformer_model=self.transformer_model)\n",
    "\n",
    "    def play_game(self, practice=True):\n",
    "        game_id, word = self.start_game(practice=practice)\n",
    "        if not game_id:\n",
    "            return False\n",
    "        \n",
    "        tries_remains = 6\n",
    "        while tries_remains > 0:\n",
    "            guess_letter = self.guess(word)\n",
    "            if not guess_letter:\n",
    "                break\n",
    "            \n",
    "            # Send the guess to the server\n",
    "            res = self.request(\"/guess_letter\", {\"game_id\": game_id, \"letter\": guess_letter})\n",
    "            status = res.get(\"status\")\n",
    "            tries_remains = res.get(\"tries_remains\", 0)\n",
    "            word = res.get(\"word\", word)\n",
    "            \n",
    "            print(f\"Game {game_id}: Guessed '{guess_letter}', Tries Remaining: {tries_remains}, Word: {word}\")\n",
    "            \n",
    "            if status == \"success\":\n",
    "                print(f\"Game {game_id}: **WON**!\")\n",
    "                return True\n",
    "            elif status == \"failed\":\n",
    "                print(f\"Game {game_id}: **LOST**!\")\n",
    "                return False\n",
    "            # Else: game is ongoing.\n",
    "        \n",
    "        print(f\"Game {game_id}: Out of tries, lost!\")\n",
    "        return False\n",
    "\n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "\n",
    "    def request(self, path, args=None):\n",
    "        args = args or {}\n",
    "        if self.access_token:\n",
    "            args[\"access_token\"] = self.access_token\n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                self.hangman_url + path,\n",
    "                params=args,\n",
    "                timeout=self.timeout,\n",
    "                verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"API request failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "# =============================================================================\n",
    "# 5.Main FN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # Replace with your actual access token and training file path.\n",
    "    training_file = r\"C:\\Users\\jaska\\OneDrive\\Desktop\\Hangman Official\\words_250000_train.txt\"  \n",
    "    access_token = \"f03eaa6b58e7172ace02888be0bf36\"  # Replace with your provided token.\n",
    "    \n",
    "    # Instantiate a transformer model (assume it has been trained or load a checkpoint).\n",
    "    transformer_model = MiniTransformerMLMTop11(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        max_len=MAX_LENGTH,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # load pretrained weights\n",
    "    transformer_model.load_state_dict(torch.load(r\"C:\\Users\\jaska\\OneDrive\\Desktop\\Hangman Official\\model_best.pt\", map_location=device))\n",
    "    \n",
    "    # Instantiate the hybrid Hangman API using our solver and transformer.\n",
    "    api = HangmanAPIHybrid(\n",
    "        access_token=access_token,\n",
    "        training_file=training_file,\n",
    "        transformer_model=transformer_model,\n",
    "        timeout=2000\n",
    "    )\n",
    "    \n",
    "    # Play a practice game:\n",
    "    success = api.play_game(practice=True)\n",
    "    print(\"Practice game result:\", \"Win\" if success else \"Loss\")\n",
    "    \n",
    "    # To check game statistics:\n",
    "    status = api.my_status()\n",
    "    print(\"Game status:\", status)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
